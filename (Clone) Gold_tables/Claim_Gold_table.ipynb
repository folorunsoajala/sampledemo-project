{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae81c313-ef67-4fc0-9153-6d5da3ba61ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First create another container (Gold layer) on your adls and create external location on databricks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ebab683-0178-452f-a6c8-26de8f6dc079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You need to add a storage path to your catalog creation command — something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba36bb7-cc29-4d77-9a02-e78bedeb629f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Catalog and Schema\n",
    "spark.sql(\"\"\"\n",
    "CREATE CATALOG IF NOT EXISTS safe_sure_catalog\n",
    "MANAGED LOCATION 'abfss://gold-layer@sampledemos.dfs.core.windows.net/'\n",
    "\"\"\")\n",
    "\n",
    "# Switch to the Catalog\n",
    "spark.sql(\"USE CATALOG safe_sure_catalog\")\n",
    "\n",
    "# Create Schema (use either DATABASE or SCHEMA — they are synonyms in Databricks)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "spark.sql(\"USE gold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a229ba5-63a8-4032-8e14-2e8adc84f4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create the agent Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a025b8a4-1fff-45af-a847-88a47cfd51d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS gold.claim (\n",
    "    ClaimID STRING,\n",
    "    PolicyID STRING,\n",
    "    ClaimDate DATE,\n",
    "    ClaimType STRING,\n",
    "    Amount DOUBLE,\n",
    "    Status STRING\n",
    ")\n",
    "USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10eb48d5-7109-463d-b467-550065c91a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from the silver layer \n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"abfss://landingzone@sampledemos.dfs.core.windows.net/InsuranceClaim.csv\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "df_clean = (df\n",
    "    .withColumn(\"ClaimDate\", F.to_date(F.col(\"ClaimDate\"), \"dd/MM/yyyy\"))\n",
    "    .withColumn(\"ClaimType\", F.initcap(F.trim(\"ClaimType\")))\n",
    "    .withColumn(\"Status\", F.when(F.lower(F.col(\"Status\")) == \"approved\", \"Approved\")\n",
    "                           .when(F.lower(F.col(\"Status\")) == \"pending\", \"Pending\")\n",
    "                           .otherwise(\"Rejected\"))\n",
    "    .withColumn(\"Amount\", F.col(\"ClaimAmount\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"gold.claim\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de28032-06a7-4bd8-aef9-6c5ef585ae4a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763556345371}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from gold.claim"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1258914090616530,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Claim_Gold_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
